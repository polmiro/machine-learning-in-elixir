# Chapter 3: Harness the power of math

```elixir
Mix.install(
  [
    {:nx, "~> 0.6.1"},
    {:exla, "~> 0.6.1"},
    {:kino, "~> 0.8"},
    {:stb_image, "~> 0.6"},
    {:vega_lite, "~> 0.1"},
    {:kino_vega_lite, "~> 0.1"}
  ],
  config: [
    nx: [
      default_backend: EXLA.Backend,
      default_defn_options: [compiler: EXLA, client: :cuda]
    ]
  ],
  system_env: [
    XLA_TARGET: "cuda118"
  ]
)

Nx.global_default_backend(EXLA.Backend)
```

## Linear algebra building blocks

Building some vectors as Nx tensors.

```elixir
a = Nx.tensor([1, 2, 3])
b = Nx.tensor([4.0, 5.0, 6.0])
c = Nx.tensor([1, 0, 1], type: {:u, 8})

IO.inspect(a, label: :a)
IO.inspect(b, label: :b)
IO.inspect(c, label: :c)
```

Building stock price vector as Nx tensor.

```elixir
goog_current_price = 2677.32
goog_pe = 23.86
goog_mkt_cap = 1760
goog = Nx.tensor([goog_current_price, goog_pe, goog_mkt_cap])
```

Scalars can be represented as plain numbers or tensors of dimension 0. Preferred way is as plain numbers.

```elixir
i_am_a_scalar = Nx.tensor(5)
i_am_also_a_scalar = 5
```

Building a list of stock prices as a matrix, package of vectors.

```elixir
meta_current_price = 133.93
meta_pe = 11.10
meta_mkt_cap = 360

stocks_matrix =
  Nx.tensor([
    [goog_current_price, goog_pe, goog_mkt_cap],
    [meta_current_price, meta_pe, meta_mkt_cap]
  ])

IO.inspect(stocks_matrix)
```

## Linear algebra operations

Vector Addition.

```elixir
sales_day_1 = Nx.tensor([32, 10, 14])
sales_day_2 = Nx.tensor([10, 24, 21])

total_sales = Nx.add(sales_day_1, sales_day_2)
```

Scalar multiplication.

```elixir
keep_rate = 0.9
unreturned_sales = Nx.multiply(keep_rate, total_sales)
```

Hadamard product.

```elixir
price_per_product = Nx.tensor([9.95, 10.95, 5.99])
revenue_per_product = Nx.multiply(unreturned_sales, price_per_product)
```

Transpose: flip matrix along its diagonal, effectively swapping rows by columns. It generalizes to higher dimensions. Notice that for 1-dimensional tensors (vectors) it is the identity function.

```elixir
sales_matrix =
  Nx.tensor([
    [32, 10, 14],
    [10, 24, 21]
  ])

Nx.transpose(sales_matrix)

vector = Nx.tensor([1, 2, 3])
Nx.transpose(vector)
```

Linear transformations (aka linear map): mapping inputs to outputs. They preserve linearity since they produce a different representation of the input preserving its properties.

```elixir
image_input = Kino.Input.image("Image", format: :jpeg)
```

content = File.read!("/path/to/image.jpeg")
Kino.Image.new(content, "image/jpeg")

```elixir
invert_color_channels =
  Nx.tensor([
    [-1, 0, 0],
    [0, -1, 0],
    [0, 0, -1]
  ])

image = Kino.Input.read(image_input)

image.data
|> StbImage.read_binary!()
|> StbImage.resize(256, 256)
|> StbImage.to_nx()
|> Nx.dot(invert_color_channels)
|> Nx.as_type({:u, 8})
|> Kino.Image.new()
```

When multiplying vectors Nx.dot/2 operation behaves as an element-wise product and then computing the sum.
 When multiplying matrix Nx.dot/2 computes the operation following the rules of matrix multiplication.

```elixir
vector = Nx.dot(Nx.tensor([1, 2, 3]), Nx.tensor([1, 2, 3])) |> IO.inspect(label: :vector)

vector_matrix =
  Nx.dot(Nx.tensor([1, 2]), Nx.tensor([[1], [2]])) |> IO.inspect(label: :vector_matrix)

matrix_matrix =
  Nx.dot(Nx.tensor([[1, 2]]), Nx.tensor([[3], [4]])) |> IO.inspect(label: :matrix_matrix)
```

## Reasoning about uncertainty

```elixir
simulation = fn key ->
  {value, key} = Nx.Random.uniform(key)
  if Nx.to_number(value) < 0.5, do: {0, key}, else: {1, key}
end

key = Nx.Random.key(42)

for n <- [10, 100, 1000, 10000] do
  Enum.map_reduce(1..n, key, fn _, key -> simulation.(key) end)
  |> elem(0)
  |> Enum.sum()
  |> IO.inspect()
end
```

Review of frequentist interpretation of probability and Bayes conditional probabilties.

Making decisions is based on goals, cost and risk associated. Not taking decision is also an option. Defining decision boundaries.

Information theory helps assess the value of information.

## Understanding differentiation

Gradient is the direction of greatest change of a scalar function.

```elixir
defmodule BerryFarm do
  import Nx.Defn

  defn profits(trees) do
    trees
    |> Nx.subtract(1)
    |> Nx.pow(4)
    |> Nx.negate()
    |> Nx.add(Nx.pow(trees, 3))
    |> Nx.add(Nx.pow(trees, 2))
  end

  defn profits_derivative(trees) do
    grad(trees, &profits/1)
  end
end

trees = Nx.linspace(0, 4, n: 100)
profits = BerryFarm.profits(trees)
profits_derivative = BerryFarm.profits_derivative(trees)

alias VegaLite, as: Vl

Vl.new(title: "Berry Profits", width: 800, height: 600)
|> Vl.data_from_values(%{
  trees: Nx.to_flat_list(trees),
  profits: Nx.to_flat_list(profits),
  profits_derivative: Nx.to_flat_list(profits_derivative)
})
|> Vl.layers([
  Vl.new()
  |> Vl.mark(:line, interpolate: :basis)
  |> Vl.encode_field(:x, "trees", type: :quantitative)
  |> Vl.encode_field(:y, "profits", type: :quantitative),
  Vl.new()
  |> Vl.mark(:line, interpolate: :basis)
  |> Vl.encode_field(:x, "trees", type: :quantitative)
  |> Vl.encode_field(:y, "profits_derivative", type: :quantitative)
  |> Vl.encode(:color, value: "#ff0000")
])
```

## Automatic differentiation with defn

Automatic differentiation is the process of computing derivatives from programs. Much more accurate than numerically aproximating gradients and more efficient than symbolic differentiation (?).

```elixir
defmodule GradFun do
  import Nx.Defn

  defn my_function(x) do
    x
    |> Nx.cos()
    |> Nx.exp()
    |> Nx.sum()
    |> print_expr()
  end

  defn grad_my_function(x) do
    grad(x, &my_function/1) |> print_expr()
  end
end

GradFun.grad_my_function(Nx.tensor([1.0, 2.0, 3.0]))
```
